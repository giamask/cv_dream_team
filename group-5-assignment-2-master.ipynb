{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":70925,"databundleVersionId":7751254,"sourceType":"competition"}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"papermill":{"default_parameters":{},"duration":151.961305,"end_time":"2022-04-12T14:50:45.298914","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-04-12T14:48:13.337609","version":"2.3.3"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"KUL H02A5a Computer Vision: Group Assignment 2\n---------------------------------------------------------------\nStudent numbers: <span style=\"color:red\">r1, r2, r0977144, r0974272, r0974660</span>.\n\nIn this group assignment your team will delve into some deep learning applications for computer vision. The assignment will be delivered in the same groups from *Group assignment 1* and you start from this template notebook. The notebook you submit for grading is the last notebook pinned as default and submitted to the [Kaggle competition](https://www.kaggle.com/t/d11be6a431b84198bc85f54ae7e2563f) prior to the deadline on **Wednesday 22 May 23:59**. Closely follow [these instructions](https://github.com/gourie/kaggle_inclass) for joining the competition, sharing your notebook with the TAs and making a valid notebook submission to the competition. A notebook submission not only produces a *submission.csv* file that is used to calculate your competition score, it also runs the entire notebook and saves its output as if it were a report. This way it becomes an all-in-one-place document for the TAs to review. As such, please make sure that your final submission notebook is self-contained and fully documented (e.g. provide strong arguments for the design choices that you make). Most likely, this notebook format is not appropriate to run all your experiments at submission time (e.g. the training of CNNs is a memory hungry and time consuming process; due to limited Kaggle resources). It can be a good idea to distribute your code otherwise and only summarize your findings, together with your final predictions, in the submission notebook. For example, you can substitute experiments with some text and figures that you have produced \"offline\" (e.g. learning curves and results on your internal validation set or even the test set for different architectures, pre-processing pipelines, etc). We advise you to first go through the PDF of this assignment entirely before you really start. Then, it can be a good idea to go through this notebook and use it as your first notebook submission to the competition. You can make use of the *Group assignment 2* forum/discussion board on Toledo if you have any questions. Good luck and have fun!\n\n---------------------------------------------------------------\nNOTES:\n* This notebook is just a template. Please keep the five main sections, but feel free to adjust further in any way you please!\n* Clearly indicate the improvements that you make! You can for instance use subsections like: *3.1. Improvement: applying loss function f instead of g*.\n","metadata":{"_cell_guid":"b47b15de-64a5-4fa9-a688-23d3efa9a2f4","_uuid":"0cc385a7-98f6-4883-96eb-7b89c7c9aa1c","papermill":{"duration":0.016533,"end_time":"2022-04-12T14:48:23.471825","exception":false,"start_time":"2022-04-12T14:48:23.455292","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# 1. Overview\nThis assignment consists of *three main parts* for which we expect you to provide code and extensive documentation in the notebook:\n* Image classification (Sect. 2)\n* Semantic segmentation (Sect. 3)\n* Adversarial attacks (Sect. 4)\n\nIn the first part, you will train an end-to-end neural network for image classification. In the second part, you will do the same for semantic segmentation. For these two tasks we expect you to put a significant effort into optimizing performance and as such competing with fellow students via the Kaggle competition. In the third part, you will try to find and exploit the weaknesses of your classification and/or segmentation network. For the latter there is no competition format, but we do expect you to put significant effort in achieving good performance on the self-posed goal for that part. Finally, we ask you to reflect and produce an overall discussion with links to the lectures and \"real world\" computer vision (Sect. 5). It is important to note that only a small part of the grade will reflect the actual performance of your networks. However, we do expect all things to work! In general, we will evaluate the correctness of your approach and your understanding of what you have done that you demonstrate in the descriptions and discussions in the final notebook.","metadata":{"_cell_guid":"35358cfb-b13d-4277-8dd5-4e663c8cd775","_uuid":"3b40b846-d7da-46d8-b354-c6d5c5ded56e","papermill":{"duration":0.014397,"end_time":"2022-04-12T14:48:23.501501","exception":false,"start_time":"2022-04-12T14:48:23.487104","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## 1.1 Deep learning resources\nIf you did not yet explore this in *Group assignment 1 (Sect. 2)*, we recommend using the TensorFlow and/or Keras library for building deep learning models. You can find a nice crash course [here](https://colab.research.google.com/drive/1UCJt8EYjlzCs1H1d1X0iDGYJsHKwu-NO).","metadata":{"papermill":{"duration":0.014263,"end_time":"2022-04-12T14:48:23.530341","exception":false,"start_time":"2022-04-12T14:48:23.516078","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom matplotlib import pyplot as plt\nimport tensorflow_hub as hub\nfrom tensorflow.keras import layers, Model\nfrom tqdm import tqdm\nimport cv2\nfrom sklearn.model_selection import StratifiedKFold\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.optimizers import Adam\nfrom PIL import Image, ImageOps\nimport scipy as scipy\nfrom tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, Activation, MaxPooling2D, UpSampling2D, concatenate\n!pip install -q git+https://github.com/tensorflow/examples.git\n!pip install iterative-stratification","metadata":{"_cell_guid":"7ddf657a-b938-4a49-87dc-b0db9af9156d","_uuid":"c65ea4f1-cc90-408f-b8e0-7c7399ec7e21","papermill":{"duration":5.416492,"end_time":"2022-04-12T14:48:28.961510","exception":false,"start_time":"2022-04-12T14:48:23.545018","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-22T11:07:53.027717Z","iopub.execute_input":"2024-05-22T11:07:53.028522Z","iopub.status.idle":"2024-05-22T11:08:42.204300Z","shell.execute_reply.started":"2024-05-22T11:07:53.028488Z","shell.execute_reply":"2024-05-22T11:08:42.203234Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-05-22 11:07:55.347088: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-22 11:07:55.347174: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-22 11:07:55.452077: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Collecting iterative-stratification\n  Downloading iterative_stratification-0.1.7-py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from iterative-stratification) (1.26.4)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from iterative-stratification) (1.11.4)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from iterative-stratification) (1.2.2)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->iterative-stratification) (1.4.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->iterative-stratification) (3.2.0)\nDownloading iterative_stratification-0.1.7-py3-none-any.whl (8.5 kB)\nInstalling collected packages: iterative-stratification\nSuccessfully installed iterative-stratification-0.1.7\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 1.2 PASCAL VOC 2009\nFor this project you will be using the [PASCAL VOC 2009](http://host.robots.ox.ac.uk/pascal/VOC/voc2009/index.html) dataset. This dataset consists of colour images of various scenes with different object classes (e.g. animal: *bird, cat, ...*; vehicle: *aeroplane, bicycle, ...*), totalling 20 classes.","metadata":{"papermill":{"duration":0.014416,"end_time":"2022-04-12T14:48:28.990998","exception":false,"start_time":"2022-04-12T14:48:28.976582","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Loading the training data\ntrain_df = pd.read_csv('//kaggle/input/kul-h02a5a-computer-vision-ga2-2024/train/train_set.csv', index_col=\"Id\")\nlabels = train_df.columns\ntrain_df[\"img\"] = [np.load('/kaggle/input/kul-h02a5a-computer-vision-ga2-2024/train/img/train_{}.npy'.format(idx)) for idx, _ in train_df.iterrows()]\ntrain_df[\"seg\"] = [np.load('/kaggle/input/kul-h02a5a-computer-vision-ga2-2024/train/seg/train_{}.npy'.format(idx)) for idx, _ in train_df.iterrows()]\nprint(\"The training set contains {} examples.\".format(len(train_df)))\n\nfig, axs = plt.subplots(2, 20, figsize=(10 * 20, 10 * 2))\nfor i, label in enumerate(labels):\n    df = train_df.loc[train_df[label] == 1]\n    axs[0, i].imshow(df.iloc[0][\"img\"], vmin=0, vmax=255)\n    axs[0, i].set_title(\"\\n\".join(label for label in labels if df.iloc[0][label] == 1), fontsize=40)\n    axs[0, i].axis(\"off\")\n    axs[1, i].imshow(df.iloc[0][\"seg\"], vmin=0, vmax=20)  # with the absolute color scale it will be clear that the arrays in the \"seg\" column are label maps (labels in [0, 20])\n    axs[1, i].axis(\"off\")\nplt.show()\n\n# The training dataframe contains for each image 20 columns with the ground truth classification labels and 20 column with the ground truth segmentation maps for each class\ntrain_df.head(1)","metadata":{"_cell_guid":"1ce67f49-6bf6-4e5c-b5e4-576e893616a9","_uuid":"3b1c5fbb-757f-4349-b224-e281c540e1ad","papermill":{"duration":21.336481,"end_time":"2022-04-12T14:48:50.342062","exception":false,"start_time":"2022-04-12T14:48:29.005581","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-22T11:08:42.206833Z","iopub.execute_input":"2024-05-22T11:08:42.208512Z","iopub.status.idle":"2024-05-22T11:09:14.614968Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"The training set contains 749 examples.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Loading the test data\ntest_df = pd.read_csv('/kaggle/input/kul-h02a5a-computer-vision-ga2-2024/test/test_set.csv', index_col=\"Id\")\ntest_df[\"img\"] = [np.load('/kaggle/input/kul-h02a5a-computer-vision-ga2-2024/test/img/test_{}.npy'.format(idx)) for idx, _ in test_df.iterrows()]\ntest_df[\"seg\"] = [-1 * np.ones(img.shape[:2], dtype=np.int8) for img in test_df[\"img\"]]\nprint(\"The test set contains {} examples.\".format(len(test_df)))\n\n# The test dataframe is similar to the training dataframe, but here the values are -1 --> your task is to fill in these as good as possible in Sect. 2 and Sect. 3; in Sect. 6 this dataframe is automatically transformed in the submission CSV!\ntest_df.head(1)","metadata":{"papermill":{"duration":11.507733,"end_time":"2022-04-12T14:49:02.044233","exception":false,"start_time":"2022-04-12T14:48:50.536500","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-22T11:09:14.616316Z","iopub.execute_input":"2024-05-22T11:09:14.616683Z","iopub.status.idle":"2024-05-22T11:09:30.691215Z","shell.execute_reply.started":"2024-05-22T11:09:14.616655Z","shell.execute_reply":"2024-05-22T11:09:30.690270Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"The test set contains 750 examples.\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"    aeroplane  bicycle  bird  boat  bottle  bus  car  cat  chair  cow  ...  \\\nId                                                                     ...   \n0          -1       -1    -1    -1      -1   -1   -1   -1     -1   -1  ...   \n\n    horse  motorbike  person  pottedplant  sheep  sofa  train  tvmonitor  \\\nId                                                                         \n0      -1         -1      -1           -1     -1    -1     -1         -1   \n\n                                                  img  \\\nId                                                      \n0   [[[139, 130, 115], [136, 127, 112], [112, 102,...   \n\n                                                  seg  \nId                                                     \n0   [[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, ...  \n\n[1 rows x 22 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>aeroplane</th>\n      <th>bicycle</th>\n      <th>bird</th>\n      <th>boat</th>\n      <th>bottle</th>\n      <th>bus</th>\n      <th>car</th>\n      <th>cat</th>\n      <th>chair</th>\n      <th>cow</th>\n      <th>...</th>\n      <th>horse</th>\n      <th>motorbike</th>\n      <th>person</th>\n      <th>pottedplant</th>\n      <th>sheep</th>\n      <th>sofa</th>\n      <th>train</th>\n      <th>tvmonitor</th>\n      <th>img</th>\n      <th>seg</th>\n    </tr>\n    <tr>\n      <th>Id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>...</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>[[[139, 130, 115], [136, 127, 112], [112, 102,...</td>\n      <td>[[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows × 22 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## 1.3 Your Kaggle submission\nYour filled test dataframe (during Sect. 2 and Sect. 3) must be converted to a submission.csv with two rows per example (one for classification and one for segmentation) and with only a single prediction column (the multi-class/label predictions running length encoded). You don't need to edit this section. Just make sure to call this function at the right position in this notebook.","metadata":{"papermill":{"duration":0.197841,"end_time":"2022-04-12T14:49:02.437252","exception":false,"start_time":"2022-04-12T14:49:02.239411","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def _rle_encode(img):\n    \"\"\"\n    Kaggle requires RLE encoded predictions for computation of the Dice score (https://www.kaggle.com/lifa08/run-length-encode-and-decode)\n\n    Parameters\n    ----------\n    img: np.ndarray - binary img array\n    \n    Returns\n    -------\n    rle: String - running length encoded version of img\n    \"\"\"\n    pixels = img.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    rle = ' '.join(str(x) for x in runs)\n    return rle\n\ndef generate_submission(df, fileName = \"submission.csv\"):\n    \"\"\"\n    Make sure to call this function once after you completed Sect. 2 and Sect. 3! It transforms and writes your test dataframe into a submission.csv file.\n    \n    Parameters\n    ----------\n    df: pd.DataFrame - filled dataframe that needs to be converted\n    \n    Returns\n    -------\n    submission_df: pd.DataFrame - df in submission format.\n    \"\"\"\n    df_dict = {\"Id\": [], \"Predicted\": []}\n    for idx, _ in df.iterrows():\n        df_dict[\"Id\"].append(f\"{idx}_classification\")\n        df_dict[\"Predicted\"].append(_rle_encode(np.array(df.loc[idx, labels])))\n        df_dict[\"Id\"].append(f\"{idx}_segmentation\")\n        df_dict[\"Predicted\"].append(_rle_encode(np.array([df.loc[idx, \"seg\"] == j + 1 for j in range(len(labels))])))\n    \n    submission_df = pd.DataFrame(data=df_dict, dtype=str).set_index(\"Id\")\n    submission_df.to_csv(fileName)\n    return submission_df","metadata":{"papermill":{"duration":0.213344,"end_time":"2022-04-12T14:49:02.848597","exception":false,"start_time":"2022-04-12T14:49:02.635253","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-22T11:09:30.694032Z","iopub.execute_input":"2024-05-22T11:09:30.694647Z","iopub.status.idle":"2024-05-22T11:09:30.704488Z","shell.execute_reply.started":"2024-05-22T11:09:30.694613Z","shell.execute_reply":"2024-05-22T11:09:30.703516Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# 2. Image classification\nThe goal here is simple: implement a classification CNN and train it to recognise all 20 classes (and/or background) using the training set and compete on the test set (by filling in the classification columns in the test dataframe).","metadata":{"papermill":{"duration":0.196641,"end_time":"2022-04-12T14:49:03.240824","exception":false,"start_time":"2022-04-12T14:49:03.044183","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class RandomClassificationModel:\n    \"\"\"\n    Random classification model: \n        - generates random labels for the inputs based on the class distribution observed during training\n        - assumes an input can have multiple labels\n    \"\"\"\n    def fit(self, X, y):\n        \"\"\"\n        Adjusts the class ratio variable to the one observed in y. \n\n        Parameters\n        ----------\n        X: list of arrays - n x (height x width x 3)\n        y: list of arrays - n x (nb_classes)\n\n        Returns\n        -------\n        self\n        \"\"\"\n        self.distribution = np.mean(y, axis=0)\n        print(\"Setting class distribution to:\\n{}\".format(\"\\n\".join(f\"{label}: {p}\" for label, p in zip(labels, self.distribution))))\n        return self\n        \n    def predict(self, X):\n        \"\"\"\n        Predicts for each input a label.\n        \n        Parameters\n        ----------\n        X: list of arrays - n x (height x width x 3)\n            \n        Returns\n        -------\n        y_pred: list of arrays - n x (nb_classes)\n        \"\"\"\n        np.random.seed(0)\n        return [np.array([int(np.random.rand() < p) for p in self.distribution]) for _ in X]\n    \n    def __call__(self, X):\n        return self.predict(X)\n    \nmodel = RandomClassificationModel()\nmodel.fit(train_df[\"img\"], train_df[labels])\ntest_df.loc[:, labels] = model.predict(test_df[\"img\"])\ntest_df.head(1)","metadata":{"papermill":{"duration":3.702597,"end_time":"2022-04-12T14:49:07.139020","exception":false,"start_time":"2022-04-12T14:49:03.436423","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-22T11:09:30.705673Z","iopub.execute_input":"2024-05-22T11:09:30.705981Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Setting class distribution to:\naeroplane: 0.06275033377837116\nbicycle: 0.0520694259012016\nbird: 0.07343124165554073\nboat: 0.06408544726301736\nbottle: 0.056074766355140186\nbus: 0.050734312416555405\ncar: 0.08411214953271028\ncat: 0.06008010680907877\nchair: 0.09212283044058744\ncow: 0.04005340453938585\ndiningtable: 0.06408544726301736\ndog: 0.05740987983978638\nhorse: 0.056074766355140186\nmotorbike: 0.06275033377837116\nperson: 0.27636849132176233\npottedplant: 0.05740987983978638\nsheep: 0.036048064085447265\nsofa: 0.05874499332443257\ntrain: 0.0534045393858478\ntvmonitor: 0.06809078771695594\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Before anything else, let's plot the distribution to get a better view of it.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(18, 6))\nplt.bar(labels, model.distribution, color='skyblue', edgecolor='black', width=1)\n\nplt.title('Label distribution')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we are dealing with a multi-label classification problem, which means we need to train a model that can recognize multiple objects/creatures in an input image, sourced from a number of categories (20 in this case).\nThe labeled part of the dataset is only 749 images long. More than 27% of the images contain a person but otherwise the dataset is fairly balanced with the rest of the categories appearing roughly 5% of the time. This means that for the majority of the categories we only have around 37 images to use in training which is a very low number of images.","metadata":{}},{"cell_type":"markdown","source":"## 2.1 Classification from scratch\n### Data augmentation\nGiven the observations we made above, it is reasonable to do some data augmentation before we move forward. Data augmentation here aims to increase the number of images available and reduce overfitting. To achieve this we create a function which transforms the images in various ways and feeds them back to the training dataset along with their labels.","metadata":{}},{"cell_type":"code","source":"IMAGE_SIZE = 128 \ndef augment(image, label, image_size):\n\n    image = tf.cast(image, tf.float32)\n    image = tf.image.random_saturation(image, lower=1, upper=3)\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    image = tf.image.random_crop(image, size=[image_size, image_size, 3])\n    image = tf.image.random_brightness(image, max_delta=0.5)\n    return image, label\n\n\n\ndef augment_matrix(images, labels, image_size):\n    assert (len(images) == len(labels))\n\n    augmented_images = []\n\n    augmented_labels = []\n\n    for i in range(len(images)):\n\n        im, l = augment(np.copy(images[i]), np.copy(labels[i]), image_size)\n        augmented_images.append(im)\n        augmented_labels.append(l)\n\n    return np.array(augmented_images), np.array(augmented_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data preparation\nHere we make sure that all the images have the same resolution. In terms of resolution, smaller images will reduce the computational load later on but the smaller we make them the more details are lost making the training process less fruitful. The average image resolution in the dataset is 471 x 383, so resizing them to 256 x 256 or 128 x 128 doesn't result in a great loss in level of detail. After testing we decided to use 128 x 128 to save on computational resources.","metadata":{}},{"cell_type":"code","source":"from skimage.transform import resize\n\nnum_classes = 20\ninput_shape = (IMAGE_SIZE, IMAGE_SIZE, 3)\n\n\ndef preprocess_image(img, target_size=(IMAGE_SIZE, IMAGE_SIZE)):\n    resized_img = resize(img, target_size, anti_aliasing=True)\n    return np.array(resized_img)\n\n\ntrain_imgs = np.array(train_df['img'])\ntrain_imgs = np.array([preprocess_image(img) for img in train_imgs])\naugmented_imgs, augmented_labels = augment_matrix(train_imgs[0:150], np.array(train_df[labels])[0:150], IMAGE_SIZE)\n\n\ntrain_imgs = np.concatenate((train_imgs, augmented_imgs),axis=0)\ntrain_labels = np.concatenate((np.array(train_df[labels]), np.array(augmented_labels)),axis=0)\n\ntest_imgs = test_df['img']\ntest_imgs = np.array([preprocess_image(img) for img in test_imgs])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's visualize some of the augmented images to make sure our alterations are working as intended.","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(1, 10, figsize=(10 * 10, 10 * 1))\nfor i in range(10):\n    axs[i].imshow(augmented_imgs[i], vmin=0, vmax=255)\n    \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Building the model\nThis is the part where we build the model to be trained. Through experimentation we have decided on the following fairly small convolutional neural network to match the size of the dataset. A brief explaination of the functionality of each layer is given below:\n\n- **Rescaling layer**. Normalizes pixel values from 0-255 to 0-1. \n- **Convolutional layer**. Responsible for the convolutional functionality at the core of each CNN. Here we have 3 such layers each followed by a pooling and a dropout layer.\n- **Pooling layer**. Applies max pooling to each 2 by 2 block of pixels, reducing the size of the input but retaining the most important features.\n- **Dropout layer**. These layers randomly set part of the input to 0 as a regularization technique that aims to reduce overfitting.\n- **Dense layer**. As is common practise with CNNs, the previous layer culminate to a series of dense layers (just two here) that map the recognized features to the labels we want to attribute to the input.\n","metadata":{}},{"cell_type":"code","source":"from tensorflow import keras\n\ndef create_cnn_model(input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3), num_classes=20):\n    model = keras.models.Sequential()\n    model.add(tf.keras.Input(shape=input_shape))\n    model.add(tf.keras.layers.Rescaling(1.0/255))\n    model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu'))   \n    model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n\n    model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n    model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n\n    model.add(tf.keras.layers.Conv2D(128, (3, 3), activation='relu'))\n    model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n\n    model.add(tf.keras.layers.Flatten())\n\n    model.add(tf.keras.layers.Dense(512, activation='relu'))\n    model.add(tf.keras.layers.Dropout(0.25))\n    # Output layer\n    model.add(tf.keras.layers.Dense(num_classes, activation='sigmoid'))\n\n    return model\n\n\nmodel = create_cnn_model()\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training the model\nFor training the model we used *stratified* cross-validation to ensure correct data distribution and checkpoints to the epoch where the best f1 score is achieved. For loss we use binary crossentropy which penalizes the model based on how well its predicted probabilities align with the true labels. Binary crossentropy is calculated for each category seperately and averaged out, essentially treating the problem as multiple classification problems.\nAs for metrics, accuracy is not a great metric for in this case because of the imbalances in the dataset and the overall low frequency of each class. A model that always predicts the majority class would have great accuracy but be completely useless. Thus, we will also add recall, precision and the aforementioned f1-score. \n\nRecall is the fraction of true positive predictions among all actual positives, precision is the percentage of true positive predictions among all positive predictions and the f1-score is a harmonic mean of both previous metrics. The f1 score is just the harmonic mean of these values.\n\nTo improve the performance of the network we also used class weights to artificially increase the impact of the under-represented labels (so every label but \"person\"), in order to balance the scales. For the same goal, we also used moving thresholds and found the per-label threshold for which the model performs best. Cross-validation is important here to ameliorate the bias introduced by this technique.","metadata":{}},{"cell_type":"code","source":"x_train, y_train = train_imgs, train_labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras import metrics\nfrom keras import callbacks as cb\nfrom sklearn.utils import class_weight\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nfrom keras import metrics\nfrom sklearn.metrics import f1_score\n\n\ndef find_best_thresholds(y_true, y_pred_probs, thresholds):\n    best_thresholds = np.zeros(y_true.shape[1])\n    for i in range(y_true.shape[1]):\n        best_f1 = 0\n        for threshold in thresholds:\n            y_pred = (y_pred_probs[:, i] >= threshold).astype(int)\n            f1 = f1_score(y_true[:, i], y_pred)\n            if f1 > best_f1:\n                best_f1 = f1\n                best_thresholds[i] = threshold\n    return best_thresholds\n\ntotal_iterations = 1\nskf = MultilabelStratifiedKFold(n_splits=10, shuffle=True, random_state=42)\nmetrics_f1=metrics.F1Score(average='micro', threshold=0.5)\nthresholds = np.linspace(0.1, 0.9, 80)\nbest_thresholds = np.zeros((10, 20))\nhistories = []\nend_results = []\navg_results = []\nfor iteration in range(total_iterations):\n    for j, (train_index, test_index) in enumerate(skf.split(x_train, y_train)):\n        X_train, X_val = x_train[train_index], x_train[test_index]\n        Y_train, Y_val = y_train[train_index], y_train[test_index]\n        \n        checkpoint = cb.ModelCheckpoint(\n            'best_model.weights.h5',\n            monitor='val_f1_score',\n            save_weights_only=True,\n            save_best_only=True,\n            mode='max',\n            verbose=0\n        )\n\n        model = create_cnn_model()\n        model.compile(optimizer='adam', loss='binary_crossentropy',\n                    metrics=['accuracy', 'recall', 'precision', metrics_f1],)\n        val_pred_probs = model.predict(X_val)\n        y = np.array(train_labels)\n        class_weights = np.zeros(20)\n        for i in range(20):\n            class_weights[i] = class_weight.compute_class_weight(\n                class_weight='balanced',\n                classes=np.array([0, 1]),\n                y=y[:, i]\n            )[1]\n\n        # Create a sample weight arrays\n        sample_weights = np.ones(y.shape[0])\n        for i in range(y.shape[0]):\n            sample_weights[i] = np.dot(y[i], class_weights)\n\n\n        # Normalize sample weights\n        sample_weights /= np.mean(sample_weights)\n        history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val),\n                            epochs=90, batch_size=64, callbacks=[checkpoint], verbose=0)\n        histories.append(history)\n\n        model.load_weights('best_model.weights.h5')\n        val_pred_probs = model.predict(X_val)\n        \n        best_thresholds[j] = find_best_thresholds(Y_val, val_pred_probs, thresholds)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now to see the cross-validation results before the moving thresholds:","metadata":{}},{"cell_type":"code","source":"avg_f1 = 0\navg_precision = 0\navg_recall = 0\nfor iteration in range(10):\n    avg_f1 += histories[iteration].history['val_f1_score'][-1]\n    avg_precision += histories[iteration].history['val_precision'][-1]\n    avg_recall += histories[iteration].history['val_recall'][-1]\nprint(\"f1 score: \",avg_f1/10)\nprint(\"precision: \",avg_precision/10)\nprint(\"recall: \",avg_recall/10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score\n\nval_pred_probs = model.predict(X_val)\naverage_best_thresholds = np.mean(best_thresholds, axis=0)\nval_pred_labels = np.array([(val_pred_probs[:, i] >= average_best_thresholds[i]).astype(\n    int) for i in range(Y_val.shape[1])]).T\nprecision_scores = [precision_score(\n    Y_val[:, i], val_pred_labels[:, i], zero_division=0.0) for i in range(num_classes)]\nrecall_scores = [recall_score(Y_val[:, i], val_pred_labels[:, i])\n                 for i in range(num_classes)]\nf1_scores = [f1_score(Y_val[:, i], val_pred_labels[:, i])\n             for i in range(num_classes)]\nprecision = np.mean(precision_scores)\nrecall = np.mean(recall_scores)\n# micro f1 - Used for comparison\nprint((2 * recall * precision) / (recall + precision))\nprint(np.mean(f1_scores))  # macro f1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Upon running one of the better training instances had the following validation results:\n\nval_f1_score: 0.23\nval_precision: 0.32\nval_recall: 0.18\n\nFrom this we can conclude that even in this better instance the results are not good, especially compared to the results achieved later on via transfer learning. As initially suspected the dataset is too small to train a model from scratch effectively.","metadata":{}},{"cell_type":"code","source":"def plot_metrics(history):\n    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n\n    axes[0, 0].plot(history.history['val_precision'],label='Validation precision')\n    axes[0, 0].plot(history.history['precision'], label='Precision')\n    axes[0, 0].set_xlabel('Epochs')\n    axes[0, 0].set_ylabel('Precision')\n    axes[0, 0].legend()\n\n    axes[0, 1].plot(history.history['val_recall'], label='Validation recall')\n    axes[0, 1].plot(history.history['recall'], label='Recall')\n    axes[0, 1].set_xlabel('Epochs')\n    axes[0, 1].set_ylabel('Recall')\n    axes[0, 1].legend()\n\n    axes[1, 0].plot(history.history['val_loss'], label='Validation Loss')\n    axes[1, 0].plot(history.history['loss'], label='Loss')\n    axes[1, 0].set_xlabel('Epochs')\n    axes[1, 0].set_ylabel('Loss')\n    axes[1, 0].legend()\n\n    axes[1, 1].plot((history.history['val_f1_score']),\n                    label='Validation f1 score')\n    axes[1, 1].plot((history.history['f1_score']), label='f1 score')\n    axes[1, 1].set_xlabel('Epochs')\n    axes[1, 1].set_ylabel('f1 score')\n    axes[1, 1].legend()\n\n    plt.tight_layout()\n    plt.show()\n\n\nplot_metrics(history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2 Training a pre-trained Classifier\n\n## 2.2.1 ResNet50V2\n\n### Data Augmentation and Model Modification\n\nTo prevent overfitting, data augmentation techniques were employed. The top layer of each pre-trained model was replaced with a custom architecture that included:\n\n- A global average pooling layer\n- A dropout layer with a rate of 0.5\n- A dense output layer with a softmax activation function\nThis custom architecture aimed to improve the generalization of the models.\n\n### Transfer Learning\n\nWe performed transfer learning over 30 epochs with a batch size of 32, using the ADAM optimizer. This phase involved training the new top layers while keeping the pre-trained layers frozen. Major improvements in model performance were observed during this phase.\n\n### Fine-tuning\n\nFine-tuning was conducted for an additional 20 epochs. During this phase, most of the pre-trained layers were kept frozen, and only the top layers were unfrozen and retrained. This approach was intended to refine the model's performance further while avoiding overfitting. However, only minor improvements were noticeable in this stage.\n\nOverall, the combination of data augmentation, a custom top layer, and strategic transfer learning and fine-tuning allowed us to achieve significant performance improvements with the pre-trained models.","metadata":{}},{"cell_type":"code","source":"def resize_images(images, img_size):\n    list_images = list()\n    for img in images:\n        img_resized = resize(img, img_size)\n        list_images.append(img_resized)\n    return np.array(list_images)\n\ndef get_y_train(): \n    y_train = list()\n    for i in range(train_df['img'].shape[0]):\n        img = train_df['img'][i]\n        lista = list()\n        for i in train_df.loc[i][:20]:\n            lista.append(i)\n        y_train.append(np.array(lista))     \n    y_train = np.array(y_train)\n    return y_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport keras\nimport time\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.applications import ResNet50V2\nfrom tensorflow.keras import layers\nimport tensorflow as tf\nfrom skimage.transform import resize\n\n### PARAMETERS\n\nNUM_CLASSES = 20\nIMAGE_SIZE = (224, 224, 3)\n\ndatagen = ImageDataGenerator(\n    rotation_range=90,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    horizontal_flip=True)\n\nepochs_pretraining = 30\nepochs_fine_tuning = 20 \ntuning_index = 185\nBATCH_SIZE = 32\n\n### LOAD DATA\n\nX_train = resize_images(train_df[\"img\"], IMAGE_SIZE)\nX_test = resize_images(test_df[\"img\"], IMAGE_SIZE)\ny_train = get_y_train()\n\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2, random_state = 4)\n\n### LOAD MODEL AND EDIT TOP CLF LAYER\n    \nresnet_model = ResNet50V2(weights='imagenet', include_top=False, pooling='avg', input_shape=IMAGE_SIZE)\nresnet_model.trainable = False\n        \nclf_model = tf.keras.Sequential() \nclf_model.add(resnet_model)\nclf_model.add(layers.Dropout(0.5))\nclf_model.add(layers.Dense(NUM_CLASSES, activation='softmax')) #output layer with NUM_CLASSES\n    \n### PRETRAINING AND TRANSFER LEARNING\n    \nclf_model.compile(optimizer='Adam', \n                  loss='binary_crossentropy', \n                  metrics=[tf.keras.metrics.BinaryAccuracy()])\n    \nhistory_pre_train = clf_model.fit(datagen.flow(X_train, y_train, batch_size=BATCH_SIZE), \n                                  validation_data=(X_val, y_val), \n                                  epochs=epochs_pretraining)\n    \n    \n### FINE TUNING\n\nfor layer in resnet_model.layers[:tuning_index]:\n    layer.trainable = False\nfor layer in resnet_model.layers[tuning_index:]:\n    layer.trainable = True\n\n\nopt = tf.keras.optimizers.Adam(learning_rate=0.0001)\nclf_model.compile(optimizer=opt,\n                  loss='binary_crossentropy', \n                  metrics=[tf.keras.metrics.BinaryAccuracy()]) \n\n\nhistory_fine_tune = clf_model.fit(datagen.flow(X_train, y_train, batch_size=BATCH_SIZE),\n                                  validation_data= (X_val, y_val),\n                                  epochs=epochs_fine_tuning)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plotting Training Curves for Transfer Learning and Fine-Tuning","metadata":{}},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n\n# Plot training accuracy on the first subplot (ax1)\nax1.plot(history_pre_train.history['binary_accuracy'], 'r')\nax1.plot(history_pre_train.history['val_binary_accuracy'], 'b')\nax1.set_xlabel(\"Iterations\")\nax1.set_ylabel(\"Accuracy\")\nax1.set_title('Training Accuracy')\nax1.set_xticks(np.arange(0, 30, 1.0)) \n\n# Plot validation accuracy on the second subplot (ax2)\nax2.plot(history_fine_tune.history['binary_accuracy'],'r')\nax2.plot(history_fine_tune.history['val_binary_accuracy'], 'b')\nax2.set_xlabel(\"Iterations\")\nax2.set_ylabel(\"Accuracy\")\nax2.set_title('Validation Accuracy')\nax2.set_xticks(np.arange(0, 20, 1.0)) \n\n #Add legends\nax1.legend(['Train', 'Validation'])\nax2.legend(['Train', 'Validation'])\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation of Precision, Recall and F-measure on Validation Set","metadata":{}},{"cell_type":"code","source":"### EVALUATE PRECISION, RECALL AND FMEASURE ON VALIDATION SET\n\ndef calculate_f1_score(precision, recall):\n    if precision + recall == 0:\n        return 0\n    else:\n        f1_score = 2 * (precision * recall) / (precision + recall)\n        return f1_score\n\nthreshold = 0.18\n\npred_y_val = clf_model.predict(X_val)\npred_y_val = np.where(pred_y_val > threshold, 1, 0)\n    \nTP = np.sum(np.logical_and(y_val == 1, pred_y_val == 1))\nFP = np.sum(np.logical_and(y_val == 0, pred_y_val == 1))\nFN = np.sum(np.logical_and(y_val == 1, pred_y_val == 0))\n\n\nprecision = TP / (TP + FP) if (TP + FP) > 0 else 0\nrecall = TP / (TP + FN) if (TP + FN) > 0 else 0 \nf1_score = calculate_f1_score(precision, recall)\n\nresult_str = \"F-measure:\\t\"+str(round(f1_score,3)) + \"\\nPrecision:\\t\" + str(round(precision,3)) + \"\\nRecall:\\t\\t\" + str(round(recall,3))\nprint(result_str)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Results on Validation Data Set\n\nThreshold: 0.18\n\n- F-Measure: 0.74\n- Precision: 0.79\n- Recall:    0.70","metadata":{}},{"cell_type":"markdown","source":"## 2.2.2 InceptionV3\n\n\nWe utilize the pre-trained InceptionV3 as the base model. The fully connected layers responsible for classification are removed from the pre-trained model. An image generator is created to perform data augmentation techniques, such as rotation, shifting, and flipping, on the training images. This helps to increase the diversity of the training data and improve the model's generalization ability.\n\nIn the initial training phase, the pre-trained layers are frozen, meaning their weights are not updated during this stage. Only the newly added dense layers are trained using the specified optimizer and binary cross-entropy loss function. This step allows the model to learn the task-specific features while preserving the general-purpose features learned by the pre-trained model.\n\nAfter the initial training phase, a specified number of layers  from the pre-trained InceptionV3 model are unfrozen, allowing their weights to be updated during the subsequent training phase. This fine-tuning process enables the model to adapt the pre-trained features to the specific task at hand. To facilitate this fine-tuning, a lower learning rate is employed, using the Stochastic Gradient Descent (SGD) optimizer with a momentum term.\n\nTo prevent overfitting and ensure optimal model performance, an early stopping mechanism is implemented. The training process is monitored based on the validation loss, and if the loss does not improve for a specified number of epochs (patience), the training is halted. The model weights corresponding to the best validation performance are restored and saved.\n\nIf early stopping does not occur during the initial training phase, the number of epochs is increased up to a maximum value of 150. This iterative process continues until either early stopping occurs or the maximum number of epochs is reached, whichever comes first.\n\nThis model reached the score of 0.37210 when submitted to evaluate classification only. ","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import InceptionV3\nfrom tensorflow.keras import layers\nimport random\nfrom skimage.transform import resize\nimport time\n\nNUM_CLASSES = 20\nIMAGE_SIZE = (224,224,3)\n\ntrain_images = train_df[\"img\"]\ntrain_labels = train_df.drop(columns=[\"img\", \"seg\"]).values\ntest_images = resize_images(test_df[\"img\"], IMAGE_SIZE)\ny_train = get_y_train()\n\n# Print shapes of training data\ntrain_labels_combined = train_labels.reshape(train_labels.shape[0], -1)\nprint(\"Shape of train_images:\", train_images.shape)\nprint(\"Shape of train_labels_combined:\", train_labels_combined.shape)\n\n# Stratify data\nstratify_column = train_labels_combined[:, 0]\nX_train, X_val, y_train, y_val = train_test_split(train_images, train_labels_combined, test_size=0.2, random_state=42, stratify=stratify_column)\n\n# Resize images\nX_train_resized = resize_images(X_train, IMAGE_SIZE)\nX_val_resized = resize_images(X_val, IMAGE_SIZE)\n\n# Data augmentation\ndata_gen = ImageDataGenerator(rotation_range=90, width_shift_range=0.2, height_shift_range=0.2, horizontal_flip=True)\n\n# Load pre-trained model\nbase_model = InceptionV3(weights='imagenet', include_top=False, input_shape=IMAGE_SIZE)\n\n# Define and compile the model\ntf_model = tf.keras.Sequential([\n    base_model,\n    layers.Flatten(),\n    layers.Dense(1024, activation='relu'),\n    layers.Dense(512, activation='relu'),\n    layers.Dropout(0.2),\n    layers.Dense(NUM_CLASSES, activation='sigmoid')\n])\n\nfor layer in base_model.layers:\n    layer.trainable = False\n\noptimizer = 'rmsprop'\ntf_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=[tf.keras.metrics.BinaryAccuracy()])\n\n# Define early stopping callback\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n\n# Train the model\ninitial_epochs = 50\nmax_epochs = 150\n\nwhile initial_epochs <= max_epochs:\n    print(f\"Training for {initial_epochs} epochs with optimizer {optimizer}\")\n\n    tf_model.fit(data_gen.flow(X_train_resized, y_train, batch_size=32),\n                 validation_data=(X_val_resized, y_val),\n                 epochs=initial_epochs,\n                 callbacks=[early_stopping])\n\n    for layer in base_model.layers[:200]:\n        layer.trainable = False\n    for layer in base_model.layers[200:]:\n        layer.trainable = True\n\n    tf_model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9),\n                     loss='binary_crossentropy',\n                     metrics=[tf.keras.metrics.BinaryAccuracy()])\n\n    start_time = time.time()\n    tf_model.fit(data_gen.flow(X_train_resized, y_train, batch_size=32),\n                 validation_data=(X_val_resized, y_val),\n                 epochs=initial_epochs,\n                 callbacks=[early_stopping])\n    elapsed_time = time.time() - start_time\n    print(\"Elapsed time: %0.2f seconds.\" % elapsed_time)\n\n\n    y_test_pred_tf = tf_model.predict(test_images)\n    test_df.loc[:, labels] = np.where(y_test_pred_tf > 0.18, 1, 0)\n\n    generate_submission(test_df, \"classification.csv\")\n\n    if early_stopping.stopped_epoch != 0:\n        break\n    else:\n        initial_epochs *= 2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del base_model\ndel tf_model\ndel clf_model\ndel model\ndel data_gen\ndel augmented_imgs \ndel augmented_labels ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Semantic segmentation\nThe goal here is to implement a segmentation CNN that labels every pixel in the image as belonging to one of the 20 classes (and/or background). Use the training set to train your CNN and compete on the test set (by filling in the segmentation column in the test dataframe).","metadata":{"papermill":{"duration":0.19763,"end_time":"2022-04-12T14:49:07.536010","exception":false,"start_time":"2022-04-12T14:49:07.338380","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Our initial dataset doesn't have enough observations for a complicated task like segmentation,\nthat's why we enriched our initial dataset with extra augmentations of the original data.\nSome augmentations requires changes also in the masks(e.g. rotations) while others don't (adding noise to the images).","metadata":{}},{"cell_type":"code","source":"# GLOBAL VARIABLES FOR THE SEGMENTATION MODEL\n# OOM ISSUES WHEN BIGGER SIZES TESTED\nIMG_TRAIN_SIZE = (224, 224)\nIMG_TRAIN_SHAPE= [224, 224, 3]\nNORMALIZE_CONST = 255","metadata":{"execution":{"iopub.status.busy":"2024-05-22T10:24:57.825498Z","iopub.status.idle":"2024-05-22T10:24:57.825828Z","shell.execute_reply.started":"2024-05-22T10:24:57.825666Z","shell.execute_reply":"2024-05-22T10:24:57.825681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Shearing is basically streching an image along an axis\n# or a reference point\ndef shear_image(img, mask, shear_factor=0.2):\n    # transform img back to [0,255]\n    img_pil = Image.fromarray((img *NORMALIZE_CONST).astype(np.uint8))\n    mask_pil = Image.fromarray(mask)\n    # shear transformation matrix\n    # the 1 is for the x coordinate which is left unchanged\n    # the shear factor is for the shear in the y coordinate based on the x coordinate\n    transformation_matrix = [\n        1, shear_factor, 0,\n        0, 1, 0]\n    # implement shears\n    # Bicubic interpolation for smooth result\n    # Nearest for preserving mask labels\n    sheared_img = img_pil.transform(img_pil.size, Image.AFFINE, transformation_matrix, Image.BICUBIC)\n    sheared_mask = mask_pil.transform(mask_pil.size, Image.AFFINE, transformation_matrix, Image.NEAREST)\n    # transform imgs back to [0,1]\n    sheared_img = np.array(sheared_img) / NORMALIZE_CONST\n    sheared_mask = np.array(sheared_mask)\n    return sheared_img, sheared_mask\n\n# rotate img 135 degrees to create irregular shapes\ndef rotate_image_135(img, mask):\n    # Rotate the image and mask by 135 degrees\n    rotated_img = scipy.ndimage.rotate(img, 135, reshape=False, mode='nearest')\n    rotated_mask = scipy.ndimage.rotate(mask, 135, reshape=False, mode='nearest')\n    # trnasform to PIL Image\n    rotated_img_pil = Image.fromarray((rotated_img * NORMALIZE_CONST).astype(np.uint8))\n    rotated_mask_pil = Image.fromarray(rotated_mask)\n    # Return the numpy arrays normalized\n    rotated_img_resized = np.array(rotated_img_pil.resize(IMG_TRAIN_SIZE, Image.NEAREST)) / NORMALIZE_CONST\n    rotated_mask_resized = np.array(rotated_mask_pil.resize(IMG_TRAIN_SIZE, Image.NEAREST))\n    return rotated_img_resized, rotated_mask_resized\n\n# Add delta to [0,1] images\ndef brightness(img, mask, delta=0.1):\n    # clip to return correct images\n    img = np.clip(img + delta, 0, 1)\n    return img, mask\n\n# Add random noise of mean 0 and var 0.01\ndef random_noise(img, mask, mean=0, var=0.01, smooth=False, sigma_smooth=1):\n    sigma = var ** 0.5\n    gaussian = np.random.normal(mean, sigma, img.shape)\n    noisy_img = img + gaussian\n    # Clip to maintain [0,1] range\n    noisy_img = np.clip(noisy_img, 0, 1)\n    return noisy_img, mask\n\n# Do gamma adjustment for augmenation\ndef gamma(img, mask, gamma=0.5):\n    # Correct gamma adjustment for normalized images\n    img = np.clip(img ** gamma, 0, 1)  # Ensure gamma output is within 0 and 1\n    return img, mask\n\n# Change hue\ndef hue(img, mask, hue_shift=-0.1):\n    # convert to [0,255]\n    img_pil = Image.fromarray((img * NORMALIZE_CONST).astype(np.uint8))\n    # Convert to hsv to change it\n    img_hsv = img_pil.convert('HSV')\n    np_img = np.array(img_hsv)\n    # Perform the hue shift\n    np_img[..., 0] = (np_img[..., 0].astype(int) + int(hue_shift * NORMALIZE_CONST)) % 256\n    # Convert back to rgb, normalize and retun\n    img_pil = Image.fromarray(np_img, 'HSV').convert('RGB')\n    img = np.array(img_pil) / NORMALIZE_CONST\n    return img, mask\n\n# zoom\ndef crop(img, mask, crop_fraction=0.7):\n    # Start/end coords of crop\n    start_crop = int((1 - crop_fraction) * img.shape[0] / 2)\n    end_crop = int(start_crop + crop_fraction * img.shape[0])\n    # Crop\n    img_cropped = img[start_crop:end_crop, start_crop:end_crop]\n    mask_cropped = mask[start_crop:end_crop, start_crop:end_crop]\n    # Resize back to correct dims\n    img_resized = np.array(Image.fromarray((img_cropped * NORMALIZE_CONST).astype(np.uint8)).resize(IMG_TRAIN_SIZE, Image.LANCZOS)) / 255.0\n    mask_resized = np.array(Image.fromarray(mask_cropped).resize(IMG_TRAIN_SIZE, Image.NEAREST))\n    return img_resized, mask_resized\n\n# Flip horizontaly\ndef flip_hori(img, mask):\n    return np.fliplr(img), np.fliplr(mask)\n\n# Flip vertically\ndef flip_vert(img, mask):\n    # Flipping does not alter the range or data type\n    return np.flipud(img), np.flipud(mask)\n\n# Rotate 90 degrees\n# k is times to rotate\ndef rotate(img, mask, k=1):\n    return np.rot90(img, k), np.rot90(mask, k)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T10:24:57.827589Z","iopub.status.idle":"2024-05-22T10:24:57.827904Z","shell.execute_reply.started":"2024-05-22T10:24:57.827750Z","shell.execute_reply":"2024-05-22T10:24:57.827763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ORIGINAL_DATASET_SIZE = train_df.shape[0]\nNUM_OF_AUGMENTATIONS = 9\nprint(f\"Original dataset size: {ORIGINAL_DATASET_SIZE}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-22T10:24:57.829414Z","iopub.status.idle":"2024-05-22T10:24:57.829732Z","shell.execute_reply.started":"2024-05-22T10:24:57.829581Z","shell.execute_reply":"2024-05-22T10:24:57.829594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Functions to perform the above augmentations\ndef create_aug_data(df, shape, train_run = False):\n    # Inialize train numpys to store data\n    train_imgs = np.zeros((ORIGINAL_DATASET_SIZE*(NUM_OF_AUGMENTATIONS+1), shape[0], shape[0], 3), dtype=np.float32)\n    train_masks = np.zeros((ORIGINAL_DATASET_SIZE*(NUM_OF_AUGMENTATIONS+1), shape[0], shape[0]), dtype=np.uint8)\n    # tqdm to show progress, zip to enumerate\n    for i, (img, mask) in enumerate(tqdm(zip(df[\"img\"], df[\"seg\"]), total=len(df))):\n        # Resize imgs/masks\n        img = cv2.resize(img, shape)\n        mask = cv2.resize(mask, shape, interpolation=cv2.INTER_NEAREST)\n        # Normalize image to range [0, 1]\n        img = img.astype(np.float32) / NORMALIZE_CONST\n        train_imgs[i] = img\n        train_masks[i] = mask\n        # To separate train and test\n        if train_run:\n        # Apply each augmentation function\n            for j, func in enumerate([gamma, brightness, hue, crop, flip_hori, flip_vert, random_noise, shear_image, rotate]):\n                aug_img, aug_mask = func(img, mask)\n                # Set it to the correct place in the array\n                train_imgs[i + ORIGINAL_DATASET_SIZE * (j + 1)] = aug_img\n                train_masks[i + ORIGINAL_DATASET_SIZE * (j + 1)] = aug_mask\n    return train_imgs, train_masks","metadata":{"execution":{"iopub.status.busy":"2024-05-22T10:24:57.831064Z","iopub.status.idle":"2024-05-22T10:24:57.831409Z","shell.execute_reply.started":"2024-05-22T10:24:57.831222Z","shell.execute_reply":"2024-05-22T10:24:57.831236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create imgs, masks\ntrain_imgs, train_masks = create_aug_data(train_df, shape=IMG_TRAIN_SIZE, train_run=True)\n# Keep only originals\ntest_imgs, _ = create_aug_data(test_df, shape=IMG_TRAIN_SIZE, train_run = False)[0:750]\ntest_imgs = test_imgs[0:750]\n# Check that it works\nassert train_imgs.shape[0] == ORIGINAL_DATASET_SIZE*(NUM_OF_AUGMENTATIONS +1)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T10:24:57.833740Z","iopub.status.idle":"2024-05-22T10:24:57.834101Z","shell.execute_reply.started":"2024-05-22T10:24:57.833932Z","shell.execute_reply":"2024-05-22T10:24:57.833949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CHECK MANUALLY THAT THE AUGMENTATIONS WORK\nfor i in range(1, 10):\n    ind = i * ORIGINAL_DATASET_SIZE\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n    # Show first image\n    axes[0].imshow(train_imgs[ind])\n    axes[0].set_title(\"Augmentation\")\n    # Show second image\n    axes[1].imshow(train_masks[ind])\n    axes[1].set_title(\"Mask\")\n  \n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-22T10:24:57.835577Z","iopub.status.idle":"2024-05-22T10:24:57.836066Z","shell.execute_reply.started":"2024-05-22T10:24:57.835810Z","shell.execute_reply":"2024-05-22T10:24:57.835829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Verify that the images are in [0,1] after augmentations\nassert(np.max(train_imgs)) == 1\nassert(np.min(train_imgs)) == 0 \nassert(np.max(test_imgs)) == 1 \nassert(np.min(test_imgs)) == 0","metadata":{"execution":{"iopub.status.busy":"2024-05-22T10:24:57.837591Z","iopub.status.idle":"2024-05-22T10:24:57.838092Z","shell.execute_reply.started":"2024-05-22T10:24:57.837815Z","shell.execute_reply":"2024-05-22T10:24:57.837835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define Dice Coefficient\ndef dice_coefficient(y_true, y_pred, smooth=1e-6):\n    # make masks one hot\n    y_true = tf.one_hot(tf.cast(y_true, tf.int32), depth=tf.shape(y_pred)[-1])\n    y_true = tf.keras.backend.flatten(y_true)\n    y_pred = tf.keras.backend.flatten(y_pred)\n    intersection = tf.keras.backend.sum(y_true * y_pred)\n    return (2. * intersection + smooth) / (tf.keras.backend.sum(y_true) + tf.keras.backend.sum(y_pred) + smooth)\n\n# Define Dice Loss as 1 - coeff\ndef dice_loss(y_true, y_pred):\n    return 1 - dice_coefficient(y_true, y_pred)\n\n# Update keras so it is serializable\ntf.keras.utils.get_custom_objects().update({\n    'dice_coefficient': dice_coefficient,\n    'dice_loss': dice_loss})","metadata":{"execution":{"iopub.status.busy":"2024-05-22T10:24:57.839573Z","iopub.status.idle":"2024-05-22T10:24:57.839923Z","shell.execute_reply.started":"2024-05-22T10:24:57.839743Z","shell.execute_reply":"2024-05-22T10:24:57.839758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to calculate the dominant class, excluding background\ndef dominant_class_excluding_background(mask, background_class=0):\n    # Filter out the background class\n    filtered_mask = mask[mask != background_class]\n    # If only background exists, return it\n    if filtered_mask.size == 0:\n        return background_class\n    # else return the biggest mask class\n    return np.argmax(np.bincount(filtered_mask))\n\n# This will be needed to stratify training and validation\nstrat_labels = np.array([dominant_class_excluding_background(mask, 0) for mask in tqdm(train_df[\"seg\"])])","metadata":{"execution":{"iopub.status.busy":"2024-05-22T10:24:57.840719Z","iopub.status.idle":"2024-05-22T10:24:57.841064Z","shell.execute_reply.started":"2024-05-22T10:24:57.840875Z","shell.execute_reply":"2024-05-22T10:24:57.840887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate pixel weights for each img\ndef calculate_pixel_weights(masks, num_classes=21):\n    n_samples, height, width = masks.shape\n    pixel_weights = np.zeros_like(masks, dtype=float)\n    for i in tqdm(range(n_samples)):\n        mask = masks[i]\n        class_weights = np.zeros(num_classes)\n        # Calculate class frequencies\n        class_counts = np.bincount(mask.flatten(), minlength=num_classes)\n        # Set weight to 0 for classes not present in the mask\n        # np.errstate is used to ignore division by zero and\n        # invalid operations (when class_counts is zero).\n        with np.errstate(divide='ignore', invalid='ignore'):\n            class_weights = 1.0 / class_counts\n            class_weights[class_counts == 0] = 0\n        # Normalize weights to sum to 1\n        total_weight = np.sum(class_weights[class_counts > 0])\n        if total_weight > 0:\n            class_weights[class_counts > 0] /= total_weight\n        pixel_weights[i] = class_weights[mask]\n    return pixel_weights\n\nweights = calculate_pixel_weights(train_masks, num_classes=21)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T10:24:57.841984Z","iopub.status.idle":"2024-05-22T10:24:57.842311Z","shell.execute_reply.started":"2024-05-22T10:24:57.842152Z","shell.execute_reply":"2024-05-22T10:24:57.842166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As a backbone we used the model from https://www.tensorflow.org/tutorials/images/segmentation with MobileNetV2 with dropout\nWe trained this heavy model also with the efficientNetB3 architecture as a backbone but the results were not optimal probably due to it being too heavy as a result we tried different architectures","metadata":{}},{"cell_type":"code","source":"from tensorflow_examples.models.pix2pix import pix2pix\nfrom tensorflow.keras.regularizers import l2\nbase_model = tf.keras.applications.MobileNetV2(input_shape=IMG_TRAIN_SHAPE, include_top=False)\n\n# We used the activations of these layers\nlayer_names = [\n    'block_1_expand_relu',   # 64x64\n    'block_3_expand_relu',   # 32x32\n    'block_6_expand_relu',   # 16x16\n    'block_13_expand_relu',  # 8x8\n    'block_16_project',      # 4x4\n]\nbase_model_outputs = [base_model.get_layer(name).output for name in layer_names]\n\n# Create the feature extraction model\ndown_stack = tf.keras.Model(inputs=base_model.input, outputs=base_model_outputs)\n# Set it to not train but use the default imgnet weights\ndown_stack.trainable = False\n\n# upsamping list\nup_stack = [\n    pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n    pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n    pix2pix.upsample(128, 3),  # 16x16 -> 32x32\n    pix2pix.upsample(64, 3),   # 32x32 -> 64x64\n]\n\n# Define model\ndef unet_model_tf(input_shape, num_classes:int, dropout_rate=0.1):\n  inputs = tf.keras.layers.Input(shape=input_shape)\n  # Downsampling through the model\n  skips = down_stack(inputs)\n  x = skips[-1]\n  skips = reversed(skips[:-1])\n  # Upsampling and establishing the skip connections\n  for up, skip in zip(up_stack, skips):\n    if dropout_rate > 0:\n        x = tf.keras.layers.Dropout(dropout_rate)(x)  # A\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Activation('relu')(x)\n    x = up(x)\n    concat = tf.keras.layers.Concatenate()\n    x = concat([x, skip])\n  # This is the last layer of the model\n  # We dont use softmax to use logits\n  last = tf.keras.layers.Conv2DTranspose(\n      filters=num_classes, kernel_size=3, strides=2,\n      padding='same')  #64x64 -> 128x128\n  x = last(x)\n  return tf.keras.Model(inputs=inputs, outputs=x)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T10:24:57.843229Z","iopub.status.idle":"2024-05-22T10:24:57.843539Z","shell.execute_reply.started":"2024-05-22T10:24:57.843381Z","shell.execute_reply":"2024-05-22T10:24:57.843393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Callback we created to display a train and a val img and mask every 7th epoch\nclass DisplayCallback(tf.keras.callbacks.Callback):\n    def __init__(self, train_data, train_masks, val_data, val_masks, num_examples=2):\n        super().__init__()\n        # Get data\n        self.train_data = train_data\n        self.train_masks = train_masks\n        self.val_data = val_data\n        self.val_masks = val_masks\n        # Number to show\n        self.num_examples = num_examples\n\n    # trigger\n    def on_epoch_end(self, epoch, logs=None):\n        # Display images\n        self.display_images(epoch)\n\n    # function to call n display\n    def display_images(self, epoch):\n        # Every 7th epoch\n        if epoch % 7 == 0 :\n            # Prefict maks\n            train_predictions = self.model.predict(self.train_data[:self.num_examples])\n            val_predictions = self.model.predict(self.val_data[:self.num_examples])\n            # Create plots\n            fig, axs = plt.subplots(self.num_examples, 6, figsize=(10, 5 * self.num_examples))  # Adjust subplot size\n            for i in range(self.num_examples):\n                # Training image\n                axs[i, 0].imshow(self.train_data[i])\n                axs[i, 0].title.set_text(f'Train Image {i+1}')\n                axs[i, 0].axis('off')\n                # Train pred mask\n                axs[i, 1].imshow(np.argmax(train_predictions[i], axis=-1), cmap='gray')\n                axs[i, 1].title.set_text(f'Predicted Train Mask {i+1}')\n                axs[i, 1].axis('off')\n                # Train mask\n                axs[i, 2].imshow(self.train_masks[i], cmap='gray')\n                axs[i, 2].title.set_text(f'Actual Train Mask {i+1}')\n                axs[i, 2].axis('off')\n\n                # Same for val\n                axs[i, 3].imshow(self.val_data[i])\n                axs[i, 3].title.set_text(f'Val Image {i+1}')\n                axs[i, 3].axis('off')\n\n                axs[i, 4].imshow(np.argmax(val_predictions[i], axis=-1), cmap='gray')\n                axs[i, 4].title.set_text(f'Predicted Val Mask {i+1}')\n                axs[i, 4].axis('off')\n\n                axs[i, 5].imshow(self.val_masks[i], cmap='gray')\n                axs[i, 5].title.set_text(f'Actual Val Mask {i+1}')\n                axs[i, 5].axis('off')\n            # Set tight layout to conserve space\n            plt.tight_layout()\n            plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-22T10:24:57.844731Z","iopub.status.idle":"2024-05-22T10:24:57.845079Z","shell.execute_reply.started":"2024-05-22T10:24:57.844892Z","shell.execute_reply":"2024-05-22T10:24:57.844918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Vanilla not pretrained Unet\n# Got 77% training accuracy so the model underfitted\ndef build_unet(input_shape, num_classes=21):\n    inputs = Input(input_shape)\n\n    # Encoder\n    conv1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)\n    conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n\n    conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)\n    conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n\n    # Decoder\n    up6 = Conv2D(64, 2, activation='relu', padding='same')(UpSampling2D(size=(2, 2))(pool2))\n    merge6 = concatenate([conv2, up6], axis=3)\n    conv6 = Conv2D(64, 3, activation='relu', padding='same')(merge6)\n    conv6 = Conv2D(64, 3, activation='relu', padding='same')(conv6)\n\n    up7 = Conv2D(32, 2, activation='relu', padding='same')(UpSampling2D(size=(2, 2))(conv6))\n    merge7 = concatenate([conv1, up7], axis=3)\n    conv7 = Conv2D(32, 3, activation='relu', padding='same')(merge7)\n    conv7 = Conv2D(32, 3, activation='relu', padding='same')(conv7)\n\n    outputs = Conv2D(num_classes, 1, activation=None)(conv7)\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-05-22T10:24:57.845921Z","iopub.status.idle":"2024-05-22T10:24:57.846258Z","shell.execute_reply.started":"2024-05-22T10:24:57.846096Z","shell.execute_reply":"2024-05-22T10:24:57.846111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train_df","metadata":{"execution":{"iopub.status.busy":"2024-05-22T10:24:57.847728Z","iopub.status.idle":"2024-05-22T10:24:57.848086Z","shell.execute_reply.started":"2024-05-22T10:24:57.847896Z","shell.execute_reply":"2024-05-22T10:24:57.847926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize cross-validation\nskf = StratifiedKFold(n_splits=5)\n# Loop over each fold\nfor fold_idx, (train_idx, val_idx) in enumerate(skf.split(train_imgs[:749], strat_labels)):\n    # set to use only first fold to prevent OOM\n    if fold_idx == 0:\n        # Save indices to extend from\n        original_train_idx = train_idx.copy()\n        original_val_idx = val_idx.copy()\n        # Calculate new indices for augmented data and add them to the original indices\n        for i in range(1, NUM_OF_AUGMENTATIONS + 1):\n            augmented_train_idx = original_train_idx + i * ORIGINAL_DATASET_SIZE\n            augmented_val_idx = original_val_idx + i * ORIGINAL_DATASET_SIZE\n            train_idx = np.concatenate((train_idx, augmented_train_idx))\n            val_idx = np.concatenate((val_idx, augmented_val_idx))\n        # Start training\n        print(f\"Training fold {fold_idx + 1}\")\n        # Split into training and validation sets\n        X_train, X_val = train_imgs[train_idx], train_imgs[val_idx]\n        y_train, y_val = train_masks[train_idx], train_masks[val_idx]\n        # Create and compile the model\n        model = unet_model_tf(input_shape=IMG_TRAIN_SHAPE, num_classes=21)\n        model.compile(\n                  optimizer=Adam(learning_rate=1e-4),\n                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n                  metrics=['accuracy'])\n        # Define callbacks\n        # Display one seen above\n        display_callback = DisplayCallback(X_train[:2], y_train[:2], X_val[:2], y_val[:2], num_examples=2)\n        # Model save checkpoint\n        checkpoint_cb_dice = ModelCheckpoint(\n            f\"best_unet_acc_fold_{fold_idx + 1}.keras\",\n            save_best_only=True,\n            monitor='val_accuracy',\n            mode='max')  # Specifically saves the model with the highest Dice coefficient\n        # Reduce LR on plateau\n        reduce_lr_cb = ReduceLROnPlateau(\n            monitor='val_accuracy',\n            factor=1/np.sqrt(10),\n            patience=5,\n            mode='max',\n            verbose=1)\n        # Early stopping callback\n        # we chose 5, 13 for the respective callbacks to enable some time\n        # for the model to train, to care for noise and local minima oscillations\n        early_stopping_cb = EarlyStopping(\n            monitor='val_accuracy',\n            patience=13,\n            mode='max',\n            restore_best_weights=True,\n            verbose=1)\n\n        # Train the model\n        model.fit(\n            X_train, y_train,\n            validation_data=(X_val, y_val),\n            epochs=100,\n            sample_weight = weights[train_idx],\n            callbacks=[checkpoint_cb_dice, reduce_lr_cb, early_stopping_cb, display_callback],\n            batch_size=8)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T10:24:57.850584Z","iopub.status.idle":"2024-05-22T10:24:57.851072Z","shell.execute_reply.started":"2024-05-22T10:24:57.850806Z","shell.execute_reply":"2024-05-22T10:24:57.850826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# If we don't have run the model before and want to load the model from memory\nif model is None:\n    model = unet_model_tf(input_shape = (224,224,3), num_classes=21)\n    model.load_weights(\"/kaggle/working/best_unet_acc_fold_1.keras\")\n# Predict test masks\npredicted_masks = model.predict(test_imgs, batch_size = 5)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T10:24:57.852515Z","iopub.status.idle":"2024-05-22T10:24:57.852848Z","shell.execute_reply.started":"2024-05-22T10:24:57.852686Z","shell.execute_reply":"2024-05-22T10:24:57.852701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create new df to get to submission\n# Takes original df and the masks\ndef create_new_df(df, masks):\n    # Resize imgs to their original\n    for i, mask in enumerate(tqdm(masks)):\n        shape1 = df[\"img\"][i].shape[0]\n        shape2 = df[\"img\"][i].shape[1]\n        temp_mask = cv2.resize(mask, (shape2, shape1), cv2.INTER_NEAREST)\n        # set it to the df making masks with 1 channel\n        df.at[i,\"seg\"] = np.argmax(temp_mask, -1)\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-05-22T10:24:57.854168Z","iopub.status.idle":"2024-05-22T10:24:57.854505Z","shell.execute_reply.started":"2024-05-22T10:24:57.854336Z","shell.execute_reply":"2024-05-22T10:24:57.854351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Carefull because here we will export only segmentation results\ntest_sub = create_new_df(test_df, predicted_masks)\nsub_df = generate_submission(test_sub, \"seg.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-05-22T10:24:57.855798Z","iopub.status.idle":"2024-05-22T10:24:57.856147Z","shell.execute_reply.started":"2024-05-22T10:24:57.855983Z","shell.execute_reply":"2024-05-22T10:24:57.855997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This code block allows to do classification\n# based on the predicted masks. We are keeping classes\n# of the mask that appears at least n times where n >= threshold\ndef find_values_above_threshold(masks, threshold):\n    # Find unique pixel values and their counts\n    unique_values, value_counts = np.unique(mask, return_counts=True)\n    # Filter values above the threshold\n    values_above_threshold = unique_values[value_counts > threshold]\n    return values_above_threshold[1:]\n\n\n# update dataframe with classes from predicted masks\n# from seg model\ndef update_with_classes(df, class_lists):\n    for i, listt in enumerate(class_lists):\n        # set all class cols to 0\n        df.iloc[i,:20] = 0\n        for element in listt:\n            # if class in list set the respective column to 1\n            df.iloc[i, element-1] = 1\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-05-22T10:24:57.857366Z","iopub.status.idle":"2024-05-22T10:24:57.857697Z","shell.execute_reply.started":"2024-05-22T10:24:57.857534Z","shell.execute_reply":"2024-05-22T10:24:57.857548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Run this block of code if you want to use the segm model as class predictor\n'''\nclass_lists = []\nfor i, mask in enumerate(tqdm(test_try[\"seg\"])):\n  class_lists.append(find_values_above_threshold(mask, 1000))\n  test_try = update_with_classes(test_try, class_lists)\n  funny_sub_df = generate_submission(test_try)\n'''","metadata":{"execution":{"iopub.status.busy":"2024-05-22T10:24:57.858848Z","iopub.status.idle":"2024-05-22T10:24:57.859462Z","shell.execute_reply.started":"2024-05-22T10:24:57.859273Z","shell.execute_reply":"2024-05-22T10:24:57.859290Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submit to competition\nYou don't need to edit this section. Just use it at the right position in the notebook. See the definition of this function in Sect. 1.3 for more details.","metadata":{"papermill":{"duration":0.196901,"end_time":"2022-04-12T14:49:19.951018","exception":false,"start_time":"2022-04-12T14:49:19.754117","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# function to export the .csv from the two csvs, one from each subteam\ndef merge_and_export(clasPath='/kaggle/working/classification.csv', segPath='/kaggle/working/seg.csv', exportedName=\"submission.csv\"):\n    df1 = pd.read_csv(clasPath, index_col=0)\n    df2 = pd.read_csv(segPath,  index_col=0)\n    merged_df = pd.merge(df1, df2, on='Id', how='inner')\n    merged_df['Predicted_x'] = merged_df['Predicted_x'].fillna(merged_df['Predicted_y'])\n    merged_df = merged_df.rename(columns={\"Predicted_x\": \"Predicted\"})\n    merged_df = merged_df.drop(columns=['Predicted_y'])\n    merged_df.to_csv(exportedName)","metadata":{"papermill":{"duration":81.176133,"end_time":"2022-04-12T14:50:41.324425","exception":false,"start_time":"2022-04-12T14:49:20.148292","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-22T10:24:57.860942Z","iopub.status.idle":"2024-05-22T10:24:57.861287Z","shell.execute_reply.started":"2024-05-22T10:24:57.861120Z","shell.execute_reply":"2024-05-22T10:24:57.861134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merge_and_export()","metadata":{"execution":{"iopub.status.busy":"2024-05-22T10:24:57.862165Z","iopub.status.idle":"2024-05-22T10:24:57.862481Z","shell.execute_reply.started":"2024-05-22T10:24:57.862320Z","shell.execute_reply":"2024-05-22T10:24:57.862333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Adversarial attack\nFor this part, your goal is to fool your classification and/or segmentation CNN, using an *adversarial attack*. More specifically, the goal is build a CNN to perturb test images in a way that (i) they look unperturbed to humans; but (ii) the CNN classifies/segments these images in line with the perturbations.","metadata":{"papermill":{"duration":0.197466,"end_time":"2022-04-12T14:50:41.721228","exception":false,"start_time":"2022-04-12T14:50:41.523762","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# get all bike imgs indexes\nclass_bike_images_indices = [i for i in range(len(train_masks[:749])) if np.isin(train_masks[i], 14).any()]\n# The number to be added incrementally\nincrement = 749\n# number of augs\ntimes = 9\n# list to put augmented indices\nextended_bike_images_indices = class_bike_images_indices.copy()\n# Generate new lists and append them\nfor i in tqdm(range(1, times + 1)):\n    # Create a new list by adding the incremental number\n    new_list = [x + i * increment for x in class_bike_images_indices]\n    # Append the new list to the extended list\n    extended_bike_images_indices += new_list\n\n# Get filtered imgs and masks\nfiltered_images = train_imgs[extended_bike_images_indices]\nfiltered_masks = train_masks[extended_bike_images_indices]\n# set every pixel where its not a bike to 0\nfiltered_masks = np.where(filtered_masks == 14, filtered_masks, 0)\n# set every bike pixel to bicycle\nfiltered_masks[filtered_masks==14] = 2","metadata":{"execution":{"iopub.status.busy":"2024-05-22T10:24:57.864220Z","iopub.status.idle":"2024-05-22T10:24:57.864545Z","shell.execute_reply.started":"2024-05-22T10:24:57.864381Z","shell.execute_reply":"2024-05-22T10:24:57.864393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_OF_AUGMENTATIONS = 9\ntotal_images = 47\n# Similar code to before to stratify the imgs for training the adversarial model\nskf = StratifiedKFold(n_splits=5)\nfor fold_idx, (train_idx, val_idx) in enumerate(skf.split(filtered_images[:total_images], strat_labels[:total_images])):\n    if fold_idx == 0:\n        original_train_idx = train_idx.copy()\n        original_val_idx = val_idx.copy()\n        for i in range(1, NUM_OF_AUGMENTATIONS + 1):\n            augmented_train_idx = original_train_idx + i * total_images\n            augmented_val_idx = original_val_idx + i * total_images\n            train_idx = np.concatenate((train_idx, augmented_train_idx))\n            val_idx = np.concatenate((val_idx, augmented_val_idx))\n        print(f\"Training fold {fold_idx + 1}\")\n        # Split into training and validation sets\n        X_train, X_val = filtered_images[train_idx], filtered_images[val_idx]\n        y_train, y_val = filtered_masks[train_idx], filtered_masks[val_idx]","metadata":{"execution":{"iopub.status.busy":"2024-05-22T10:24:57.865694Z","iopub.status.idle":"2024-05-22T10:24:57.866039Z","shell.execute_reply.started":"2024-05-22T10:24:57.865847Z","shell.execute_reply":"2024-05-22T10:24:57.865860Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Our adversarial model\ndef build_adversarial_model(input_shape):\n    inputs = Input(shape=input_shape)\n    # Encoder path\n    x = Conv2D(8, (3, 3), activation='relu', padding='same')(inputs)\n    x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n    # Decoder path\n    x = Conv2DTranspose(32, (3, 3), activation='relu', padding='same')(x)\n    x = Conv2DTranspose(64, (3, 3), activation='relu', padding='same')(x)\n    # Output perturbation as the same size as input\n    perturbations = Conv2D(3, (3, 3), padding='same')(x)\n    model = Model(inputs=inputs, outputs=perturbations)\n    return model\n\n# Adversarial loss\ndef adversarial_loss(labels, perturbations, images, scale=0.1):\n    # Apply perturbations to the original images\n    perturbed_images = images + perturbations\n    # Get predictions from the target model on the perturbed images\n    predictions = target_model(perturbed_images, training=False)\n    # Convert predictions from logits to class indices\n    predicted_classes = tf.argmax(predictions, axis=-1)\n    # Cast labels to match the predicted_classes dtype\n    labels = tf.cast(labels, dtype=tf.int32)\n    predicted_classes = tf.cast(predicted_classes, dtype=tf.int32)\n    # Calculate  corrrect preds and acc\n    correct_predictions = tf.equal(labels, predicted_classes)\n    accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n    # euclidean regularization to keep perturbations small\n    loss_regularization = tf.norm(perturbations, ord=2)\n    # Combine losses, adapted as a form of loss where lower accuracy increases the \"loss\"\n    # to make the model learn the masks\n    # also tried with reverse dice\n    # and reverse sparse\n    # but they all failed\n    total_loss = (1 - accuracy) + scale * loss_regularization\n    return total_loss\n\n# get batches to train and val\n# set it to 10 since it's the number of augs\n# and will always be divisible\ndef get_batches(X, y, batch_size=10):\n    num_samples = X.shape[0]\n    for start_idx in range(0, num_samples, batch_size):\n        end_idx = min(start_idx + batch_size, num_samples)\n        # yield to be able to continue\n        yield X[start_idx:end_idx], y[start_idx:end_idx]\n\n# Instantiate models\nadv_model = build_adversarial_model(input_shape=(224, 224, 3))\ntarget_model = unet_model_tf(input_shape=(224, 224, 3), num_classes=21)\ntarget_model.load_weights(\"/kaggle/working/best_unet_acc_fold_1.keras\")\ntarget_model.trainable = False\nadv_model.compile(optimizer=Adam(learning_rate=1e-4))\n\n# Set hyperparams\nepochs = 10\nbatch_size = 2\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n\n# training step\n@tf.function\ndef train_step(images, labels, model, optimizer):\n    # initiate gradient\n    with tf.GradientTape() as tape:\n        # get perturbations and add em and calc loss\n        perturbations = model(images, training=True)\n        perturbed_images = images + perturbations\n        loss = adversarial_loss(labels, perturbations, images)\n    # calc gradients\n    gradients = tape.gradient(loss, model.trainable_variables)\n    # update weights\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    return loss\n\n\n# Training loop\nfor epoch in range(epochs):\n    print(f\"Epoch {epoch+1}/{epochs}\")\n    train_loss = 0\n    train_steps = 0\n    for images, labels in tqdm(get_batches(X_train, y_train, batch_size), desc=\"Training\"):\n        # Ensure labels are of type int32\n        # Else we got errors\n        labels = tf.cast(labels, tf.int32)\n        loss = train_step(images, labels, adv_model, optimizer)\n        train_loss += loss.numpy()\n        train_steps += 1\n\n        # Displaying one training image per epoch\n        if train_steps == 1:\n            perturbations = adv_model(images, training=False)\n            perturbed_images = images + perturbations\n            perturbed_images = tf.clip_by_value(perturbed_images, 0, 1)\n            predictions = target_model(perturbed_images)\n            predicted_classes = tf.argmax(predictions, axis=-1)\n\n            plt.figure(figsize=(15, 10))\n            plt.subplot(1, 4, 1)\n            plt.imshow(images[0])\n            plt.title(f'Original Image predicting class {np.max(labels[0])}')\n            plt.axis('off')\n\n            plt.subplot(1, 4, 2)\n            plt.imshow(perturbations[0]*255, cmap='viridis')\n            plt.title('Perturbation')\n            plt.axis('off')\n\n            plt.subplot(1, 4, 3)\n            plt.imshow(perturbed_images[0])\n            plt.title('Perturbed Image')\n            plt.axis('off')\n\n            plt.subplot(1, 4, 4)\n            plt.imshow(predicted_classes[0], cmap='viridis')\n            plt.title(f'Predicted Mask predicting class {np.max(predicted_classes[0][predicted_classes[0] != 0])}')\n            plt.axis('off')\n\n            plt.show()\n    train_loss /= train_steps\n    print(f\"Training loss: {train_loss}\")\n\n    # Validation loop with tqdm progress bar\n    val_loss = 0\n    val_steps = 0\n    for images, labels in tqdm(get_batches(X_val, y_val, batch_size), desc=\"Validation\"):\n        # No training, just forward pass\n        labels = tf.cast(labels, tf.int32)\n        # Displaying one val image per epoch\n        if train_steps == 1:\n            perturbations = adv_model(images, training=False)\n            perturbed_images = images + perturbations\n            # Clip values to [0, 1] range\n            perturbed_images = tf.clip_by_value(perturbed_images, 0, 1)\n            predictions = target_model(perturbed_images)\n            predicted_classes = tf.argmax(predictions, axis=-1)\n\n            plt.figure(figsize=(15, 10))\n            plt.subplot(1, 4, 1)\n            plt.imshow(images[0])\n            plt.title(f'Original Image predicting class {np.max(labels[0])}')\n            plt.axis('off')\n\n            plt.subplot(1, 4, 2)\n            plt.imshow(perturbations[0]*255, cmap='viridis')\n            plt.title('Perturbation')\n            plt.axis('off')\n\n            plt.subplot(1, 4, 3)\n            plt.imshow(perturbed_images[0])\n            plt.title('Perturbed Image')\n            plt.axis('off')\n\n            plt.subplot(1, 4, 4)\n            plt.imshow(predicted_classes[0], cmap='viridis')\n            plt.title(f'Predicted Mask predicting class {np.max(predicted_classes[0][predicted_classes[0] != 0])}')\n            plt.axis('off')\n            plt.show()\n\n        loss = adversarial_loss(labels, perturbations, images)\n        val_loss += loss.numpy()\n        val_steps += 1\n    val_loss /= val_steps\n    print(f\"Validation loss: {val_loss}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-22T10:24:57.867273Z","iopub.status.idle":"2024-05-22T10:24:57.867603Z","shell.execute_reply.started":"2024-05-22T10:24:57.867440Z","shell.execute_reply":"2024-05-22T10:24:57.867455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Discussion\nFinally, take some time to reflect on what you have learned during this assignment. Reflect and produce an overall discussion with links to the lectures and \"real world\" computer vision.","metadata":{"papermill":{"duration":0.195695,"end_time":"2022-04-12T14:50:42.117581","exception":false,"start_time":"2022-04-12T14:50:41.921886","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Segmentation\nOn segmentation we tried only a vanilla unet for the untrained one and one with a pretrained backbone. After seeing the advanced results for the pretrained one, we focused on developing several ones with available models as efficientnetV2B3, efficientNetB3 and several versions of mobileNet, however the V2 showed the best results. What we found quite surprising at first was the relative accuracy that the model could achieve in classification as well as segmentation by tinkering with the threshold parameter. Given mode time, we would create an accuracy curve for the different thershold we could implement and coordinate with the classification team to compare results","metadata":{}},{"cell_type":"markdown","source":"## Adversarial segmentation\n\nNo matter what classes we tried to juggle, very similar ones such as bike and bicycle or ones that were similar and appeared often alone such as cat and dog, the results we got were subpar. The model underfitted at all times, and some extra loss functions we tried, scaling factors and slightly smaller or bigger models yielded similar results.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}